---
version: '2.2'

networks:
  zookeeper:
    driver: bridge
    internal: true
  kafka:
    driver: bridge
    internal: true
  api:
    driver: bridge
    internal: true
  mail:
    driver: bridge
    internal: true
  monitor:
    driver: bridge
    internal: true
  security:
    driver: bridge
    internal: true
  hadoop:
    external: true

volumes:
  kafka-1-data: {}
  kafka-2-data: {}
  kafka-3-data: {}
  zookeeper-1-data: {}
  zookeeper-2-data: {}
  zookeeper-3-data: {}
  zookeeper-1-log: {}
  zookeeper-2-log: {}
  zookeeper-3-log: {}
  radar-backend-monitor-disconnect-data: {}
  keycloak-exports: {}
  certs:
    external: true
  certs-data:
    external: true

services:
  #---------------------------------------------------------------------------#
  # Zookeeper Cluster                                                         #
  #---------------------------------------------------------------------------#
  zookeeper-1:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - /data/zookeeper-1-data:/var/lib/zookeeper/data
      - /data/zookeeper-1-log:/var/lib/zookeeper/logs
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-1 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  zookeeper-2:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - /data/zookeeper-2-data:/var/lib/zookeeper/data
      - /data/zookeeper-2-log:/var/lib/zookeeper/logs
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-2 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  zookeeper-3:
    image: confluentinc/cp-zookeeper:4.1.0
    networks:
      - zookeeper
    volumes:
      - /data/zookeeper-3-data:/var/lib/zookeeper/data
      - /data/zookeeper-3-log:/var/lib/zookeeper/logs
    restart: always
    environment:
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    healthcheck:
      test: ["CMD", "/bin/bash", "-c", "[ $$(echo dump | nc zookeeper-3 2181 | head -c1 | wc -c) -gt 0 ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Kafka Cluster                                                             #
  #---------------------------------------------------------------------------#
  kafka-1:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - /data/kafka-1-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/1 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  kafka-2:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - /data/kafka-2-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/2 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  kafka-3:
    image: confluentinc/cp-kafka:4.1.0
    networks:
      - kafka
      - zookeeper
    volumes:
      - /data/kafka-3-data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    restart: always
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 730
      KAFKA_MESSAGE_MAX_BYTES: 4000048
      KAFKA_LOG4J_LOGGERS: kafka.producer.async.DefaultEventHandler=INFO,kafka.controller=INFO,state.change.logger=INFO
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: "1.1"
      KAFKA_LOG_MESSAGE_FORMAT_VERSION: "1.1"
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080
    healthcheck:
      test: ["CMD-SHELL", "echo dump | nc zookeeper-1 2181 | grep -q /brokers/ids/3 || exit 1"]
      interval: 1m30s
      timeout: 10s
      retries: 3

  #---------------------------------------------------------------------------#
  # Schema Registry                                                           #
  #---------------------------------------------------------------------------#
  schema-registry-1:
    image: confluentinc/cp-schema-registry:4.1.0
    networks:
      - kafka
      - zookeeper
      - api
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    restart: always
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper-1:2181
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8081/subjects"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # REST proxy                                                                #
  #---------------------------------------------------------------------------#
  rest-proxy-1:
    image: confluentinc/cp-kafka-rest:4.1.0
    networks:
      - kafka
      - zookeeper
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
    restart: always
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_REST_LISTENERS: http://0.0.0.0:8082
      KAFKA_REST_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      KAFKA_REST_HOST_NAME: rest-proxy-1
      KAFKA_REST_COMPRESSION_TYPE: lz4
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8082/topics"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # Kafka Init                                                                #
  #---------------------------------------------------------------------------#
  kafka-init:
    build:
      context: images/radar-kafka-init
      args:
        SCHEMAS_VERSION: ${RADAR_SCHEMAS_VERSION}
    image: radarbase/kafka-init:${RADAR_SCHEMAS_VERSION}
    networks:
      - kafka
      - zookeeper
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
    volumes:
      - ./etc/schema:/schema/conf
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_NUM_BROKERS: 3
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_NUM_REPLICATION: 3

  #---------------------------------------------------------------------------#
  # RADAR Cold Storage                                                        #
  #---------------------------------------------------------------------------#
  hdfs-datanode-1:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    hostname: hdfs-datanode-1
    command: datanode
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_1}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-datanode-2:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: datanode
    hostname: hdfs-datanode-2
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_2}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-datanode-3:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: datanode
    hostname: hdfs-datanode-3
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
    volumes:
      - "${HDFS_DATA_DIR_3}:/hadoop/dfs/data"
    restart: always
    environment:
      SERVICE_9866_NAME: datanode
      SERVICE_9867_IGNORE: "true"
      SERVICE_9864_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_REPLICATION: 2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  hdfs-namenode-1:
    build:
      context: ./images/hdfs
      args:
        BASE_VERSION: ${HDFS_BASE_VERSION}
    image: radarbase/hdfs:${HDFS_BASE_VERSION}
    command: namenode-1
    hostname: hdfs-namenode-1
    networks:
      - hadoop
    volumes:
      - "${HDFS_NAME_DIR_1}:/hadoop/dfs/name/1"
      - "${HDFS_NAME_DIR_2}:/hadoop/dfs/name/2"
    restart: always
    environment:
      SERVICE_8020_NAME: namenode
      SERVICE_9870_IGNORE: "true"
      HADOOP_HEAPSIZE: 1000
      HADOOP_NAMENODE1_HOSTNAME: hdfs-namenode-1
      HADOOP_DFS_NAME_DIR: file:///hadoop/dfs/name/1,file:///hadoop/dfs/name/2
    healthcheck:
      test: ["CMD", "hdfs", "dfs", "-test", "-e", "/"]
      interval: 1m
      timeout: 15s
      retries: 3

  #---------------------------------------------------------------------------#
  # Email server                                                              #
  #---------------------------------------------------------------------------#
  smtp:
    image: namshi/smtp:latest
    networks:
      - mail
      - default
    volumes:
      - /var/spool/exim
    restart: always
    env_file:
      - ./etc/smtp.env

  #---------------------------------------------------------------------------#
  # RADAR HDFS connector                                                     #
  #---------------------------------------------------------------------------#
  radar-hdfs-connector:
    image: radarbase/radar-connect-hdfs-sink:0.2.1
    restart: on-failure
    volumes:
      - ./etc/hdfs-connector/sink-hdfs.properties:/etc/kafka-connect/sink-hdfs.properties
    networks:
      - zookeeper
      - kafka
      - hadoop
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - kafka-init
      - hdfs-datanode-1
      - hdfs-datanode-2
      - hdfs-datanode-3
      - hdfs-namenode-1
    environment:
      CONNECT_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,PLAINTEXT://kafka-2:9092,PLAINTEXT://kafka-3:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "default"
      CONNECT_CONFIG_STORAGE_TOPIC: "default.config"
      CONNECT_OFFSET_STORAGE_TOPIC: "default.offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "default.status"
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry-1:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_OFFSET_STORAGE_FILE_FILENAME: "/tmp/connect2.offset"
      CONNECT_REST_ADVERTISED_HOST_NAME: "radar-hdfs-connector"
      CONNECT_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      CONNECTOR_PROPERTY_FILE_PREFIX: "sink-hdfs"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx768m"
      KAFKA_BROKERS: 3
      CONNECT_LOG4J_LOGGERS: "org.reflections=ERROR"
    healthcheck:
      test: ["CMD-SHELL", "curl  -sf localhost:8083/connectors/radar-hdfs-sink-android-15000/status | grep -o '\"state\":\"[^\"]*\"' | tr '\\n' ',' | grep -vq FAILED || exit 1"]
      interval: 1m
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR backend monitor                                                     #
  #---------------------------------------------------------------------------#
  radar-backend-monitor:
    image: radarbase/radar-backend:0.4.0
    command: monitor
    networks:
      - zookeeper
      - kafka
      - mail
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry-1
      - kafka-init
      - smtp
    volumes:
      - ./etc/radar-backend/radar.yml:/etc/radar.yml
      - /data/radar-backend-monitor-disconnect-data:/var/lib/radar/data
    restart: always
    environment:
      KAFKA_REST_PROXY: http://rest-proxy-1:8082
      KAFKA_SCHEMA_REGISTRY: http://schema-registry-1:8081
      KAFKA_BROKERS: 3
      # For backwards compatibility
      TOPIC_LIST: "application_record_counts"

  #---------------------------------------------------------------------------#
  # Docker Monitoring                                                         #
  #---------------------------------------------------------------------------#
  portainer:
    image: portainer/portainer:1.23.1
    command: --admin-password '${PORTAINER_PASSWORD_HASH}'
    networks:
      - monitor
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    restart: always

  #---------------------------------------------------------------------------#
  # Webserver                                                                 #
  #---------------------------------------------------------------------------#
  webserver:
    image: nginx:1.14.0-alpine
    restart: always
    networks:
      - api
      - monitor
      - default
    depends_on:
      - portainer
      - schema-registry-1
      - gateway
      - kafka-manager
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /data/certs:/etc/letsencrypt
      - /data/certs-data:/data/letsencrypt
      - "./etc/webserver/nginx.conf:/etc/nginx/nginx.conf:ro"
      - "./etc/webserver/cors.conf:/etc/nginx/cors.conf:ro"
      - "./etc/webserver/ip-access-control.conf:/etc/nginx/ip-access-control.conf:ro"
      - "./etc/webserver/kafka-manager.htpasswd:/etc/nginx/kafka-manager.htpasswd:ro"
      - "./etc/webserver/optional-services.conf:/etc/nginx/optional-services.conf"
      - "${APP_ACCESS_LOGS_DIR}/:/data/log/"
    # healthcheck hard to do, however, it is possible to monitor this externally
    # with
    # docker logs --since 2m radarcphadoopstack_webserver_1 | grep "connect() failed"


  radarbase-postgresql:
    image: postgres:${POSTGRES_VERSION}
    networks:
      - security
    volumes:
      - "${RB_POSTGRES_DIR}/data/:/var/lib/postgresql/data/"
      - "./postgres-backup/backups/postgresql:/backups/database/postgresql/"
      - "./postgres-backup/scripts:/backup-scripts"
    environment:
      POSTGRES_USER : ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: rbkeycloak
    healthcheck:
      test: ["CMD-SHELL", "PGPASSWORD='${POSTGRES_PASSWORD}' psql -U '${POSTGRES_USER}' rbkeycloak -l || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  radarbase-keycloak:
    image: jboss/keycloak:4.8.3.Final
    networks:
      - api
      - security
    volumes:
      - ./images/keycloak/theme/ucl:/opt/jboss/keycloak/themes/ucl
    environment:
      KEYCLOAK_USER : ${KEYCLOAK_USER}
      KEYCLOAK_PASSWORD: ${KEYCLOAK_PASSWORD}
      DB_VENDOR: postgres
      DB_ADDR: radarbase-postgresql
      DB_PORT: 5432
      DB_DATABASE: rbkeycloak
      DB_USER: ${POSTGRES_USER}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      PROXY_ADDRESS_FORWARDING: "true"
    depends_on:
      - radarbase-postgresql
    restart: always

  #---------------------------------------------------------------------------#
  # Kafka Manager                                                             #
  #---------------------------------------------------------------------------#
  kafka-manager:
    image: radarbase/kafka-manager:1.3.3.18
    networks:
      - zookeeper
      - kafka
      - api
    depends_on:
      - zookeeper-1
      - kafka-1
      - kafka-2
      - kafka-3
    environment:
      ZK_HOSTS: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "[ $$(wget -q -O - localhost:9000/kafkamanager/api/health) = healthy ] || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  #---------------------------------------------------------------------------#
  # RADAR Gateway                                                             #
  #---------------------------------------------------------------------------#
  gateway:
    image: radarbase/radar-gateway:0.3.9
    networks:
      - api
      - kafka
    depends_on:
      - rest-proxy-1
    volumes:
      - ./etc/gateway:/etc/radar-gateway
    command: ["radar-gateway", "/etc/radar-gateway/gateway.yml"]
    healthcheck:
      # should give an unauthenticated response, rather than a 404
      test: ["CMD-SHELL", "wget --spider localhost/radar-gateway/topics 2>&1 | grep -q 401 || exit 1"]
      interval: 1m30s
      timeout: 5s
      retries: 3

  sftp:
    image: mighealth/sftp:alpine
    build: images/sftp
    networks:
      - default
    ports:
      - "2222:2222"
    volumes:
      - ${RESTRUCTURE_OUTPUT_DIR}:/data/data:ro
      - ${APP_ACCESS_LOGS_DIR}/:/data/log:ro
      - keycloak-exports:/data/keycloak:ro
      - ./etc/sftp/users.conf:/etc/sftp/users.conf:ro
      - ./etc/sftp/authorized_keys:/etc/sftp/authorized_keys:ro
      - ./etc/sftp/ssh_host_ed25519_key:/etc/ssh/ssh_host_ed25519_key:ro
      - ./etc/sftp/ssh_host_rsa_key:/etc/ssh/ssh_host_rsa_key:ro


  radar-output:
    image: radarbase/radar-hdfs-restructure:0.6.0
    restart: always
    stop_signal: SIGINT
    networks:
      - hadoop
    depends_on:
      - hdfs-namenode-1
      - hdfs-datanode-1
      - hdfs-datanode-2
      - hdfs-datanode-3
    volumes:
      - ${RESTRUCTURE_OUTPUT_DIR}:/output
      - ./etc/hdfs-restructure/restructure.yml:/etc/restructure.yml
    environment:
      RADAR_HDFS_RESTRUCTURE_OPTS: -Xms250m -Xmx2g
    command: -F /etc/restructure.yml -S
